{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the Epic Sci-Fi Universe\n",
    "# Notebook-3 (Modeling)\n",
    "### Perry Shyr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook covers my baseline model and five classifier-models.  A pipeline with GridSearching is used to find optimum hyperparameters with the first model created (logistic regression).  A GridSearch is also used to find the best hyperparameters for the fourth model created (random-forests).  The test split comprised of 458 posts is used in the scoring calculations of each model.\n",
    "\n",
    "#### 1. Logistic Regression model\n",
    "#### 2. Naive-Bayes (multinomial) model\n",
    "#### 3. k-Nearest Neighbors model\n",
    "#### 4. Random-forests model\n",
    "#### 5. Support-vector Machine model\n",
    "\n",
    "### Note: I choose to go with the TF-IDF vectorizer for each of my NLP models.  I did explore additional analysis using SVD, but the component results provided little to no information.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries and data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm, linear_model, datasets\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score, make_scorer, f1_score, precision_score\n",
    "from sklearn.metrics import classification_report, roc_curve\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I import the data collected without the \"Star\" or \"star\" when the franchise title is referenced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/combined_no_star.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class sizes:\n",
    "#### My positive-class (\"1\") is the group of post-titles from the 'r/startrek' subreddit.  The class balance is fairly even."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    52.925096\n",
       "0    47.074904\n",
       "Name: is_trek, dtype: float64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['is_trek'].value_counts(normalize=True)*100    #  As percentages, my classes are almost evenly balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BASELINE: My baseline model states that any given post originated from the 'r/startrek' subreddit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test-Split:\n",
    "#### This is used in preparation for cross-validation, and in measuring how well a model generalizes on untrained data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data['title']\n",
    "y=data['is_trek']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y, \n",
    "                                                    random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The split results are saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('../data/X_train.csv', index=False)\n",
    "X_test.to_csv('../data/X_test.csv', index=False)\n",
    "y_train.to_csv('../data/y_train.csv', index=False)\n",
    "y_test.to_csv('../data/y_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch over TFIDF/Log-Reg pipeline:\n",
    "#### This pipeline sets up a sequence of vectorization and modeling.  It allows me to search over various hyperparameters in the two steps (vectorizing and modeling.  The model used in this pipeline is logistic regresssion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "    ('logistic', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       " ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "star_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9905178701677607"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "star_pipe.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9104803493449781"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "star_pipe.score(X_test, y_test)      # This modeling is overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The high training fit is not unexpected.  There is an 8% drop in accuracy representing high variance and unnecessary complexity.  That is, ideally some features should be removed from the modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_params = {\n",
    "    'tfidf__min_df': [1,2,3],\n",
    "    'tfidf__max_df': np.linspace(.2,.45,10),\n",
    "    'logistic__C': np.linspace(0.5,2.0,15),\n",
    "    'logistic__penalty': ['l1','l2']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(star_pipe, star_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The hyperparameters we are searching over include minimum-document frequency, maximum-document frequency in the vectorization, and 'C' (regularization-inverse) and regulatization penalty in logistic regression.  We intentionally excluded a 'min_df' of 1, which means that features that only appear in a single document are given weight.  The upper range for 'max_df' searched is unusually low, which means that features are not weighted if they occur more often than 35% of the corpus.  Although we searched over through the vectorizing process, I may yet decide to commit to using the results going forwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       " ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'tfidf__min_df': [1, 2, 3], 'tfidf__max_df': array([0.2    , 0.22778, 0.25556, 0.28333, 0.31111, 0.33889, 0.36667,\n",
       "       0.39444, 0.42222, 0.45   ]), 'logistic__C': array([0.5    , 0.60714, 0.71429, 0.82143, 0.92857, 1.03571, 1.14286,\n",
       "       1.25   , 1.35714, 1.46429, 1.57143, 1.67857, 1.78571, 1.89286,\n",
       "       2.     ]), 'logistic__penalty': ['l1', 'l2']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9956236323851203"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9082969432314411"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_test, y_test)                     # This modeling is also overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logistic__C': 1.5714285714285714,\n",
       " 'logistic__penalty': 'l2',\n",
       " 'tfidf__max_df': 0.2,\n",
       " 'tfidf__min_df': 1}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The search yielded 1.57 for 'C,' 'Ridge' for the regularization, 1 for the 'min_df' and probably less than 20% for 'max_df' because this is the low-end of my range.  It fit under 99% of the training dataset and scored almost 91% of the test dataset.  The logistic-regression model is definitely overfit.  The 'min_df' of 1 and 'max_df' of under 20% are practically absurd values, so will be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I include a CountVectorization step here to get an idea of word frequencies.  I set the 'binary' option to 'true just to count document frequency and not frequencies within each document.  This data may be useful during my model evaluation (next notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(binary='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cv = cvec.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the             719\n",
       "of              343\n",
       "to              326\n",
       "in              313\n",
       "trek            275\n",
       "and             254\n",
       "for             225\n",
       "is              187\n",
       "on              170\n",
       "my              165\n",
       "what            163\n",
       "wars            160\n",
       "this            155\n",
       "you             137\n",
       "it              137\n",
       "new             127\n",
       "with            122\n",
       "that            107\n",
       "series          105\n",
       "picard          103\n",
       "was             100\n",
       "from             97\n",
       "about            96\n",
       "have             88\n",
       "be               84\n",
       "just             83\n",
       "do               82\n",
       "at               82\n",
       "tng              76\n",
       "discovery        75\n",
       "               ... \n",
       "naked             1\n",
       "nails             1\n",
       "mythology         1\n",
       "mysterious        1\n",
       "myself            1\n",
       "needed            1\n",
       "neepers           1\n",
       "norton            1\n",
       "nightmare         1\n",
       "northeast         1\n",
       "north             1\n",
       "normally          1\n",
       "normal            1\n",
       "nor               1\n",
       "noise             1\n",
       "nodes             1\n",
       "nm                1\n",
       "nintendo          1\n",
       "niece             1\n",
       "negotiations      1\n",
       "nicotine          1\n",
       "nicole            1\n",
       "nexus             1\n",
       "newbie            1\n",
       "neverforget       1\n",
       "network           1\n",
       "net               1\n",
       "neon              1\n",
       "neighborhood      1\n",
       "000               1\n",
       "Length: 4151, dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cvec  = pd.DataFrame(X_cv.todense(),\n",
    "                   columns=cvec.get_feature_names())\n",
    "df_cvec.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When I sort by document frequency, I see a lot of stop words.\n",
    "### I save the count results and note how huge the file is that is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cvec.to_csv('../data/cvec.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the GridSearch above, the critical hyperparameter that I find is \"min_df\" for the Vectorizer.  Although a 'min_df' value of \"1\" gives the best score, it makes no sense, so I choose to use a 'min_df' value of \"2\" going forwards despite the loss in accuracy.  The 'max_df' value of \"0.2\" is unbelievably low, so  I decide to use a value of \"0.5.\"\n",
    "\n",
    "#### I also choose to include bigrams in the models below to capture features like \"Death Star\" or \"USS Enterprise.\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The TFIDF-Vectorization is run here as the consistent input of each of my five separate models, for comparability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english', min_df=2, max_df=.5, ngram_range=(1,2))\n",
    "X_train_transform = tfidf.fit_transform(X_train)\n",
    "X_test_transform = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The vectorization is saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickles/p3_xtrain_transform.pkl', 'wb+') as f:\n",
    "    pickle.dump(X_train_transform, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickles/p3_xtest_transform.pkl', 'wb+') as f:\n",
    "    pickle.dump(X_test_transform, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickles/tfidf.pkl', 'wb+') as f:\n",
    "    pickle.dump(tfidf, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I plan to test five separate models for comparison purposes before deciding which two to evaluate in detail.  \n",
    "### These include Logistic-regression, Multinomial-Naive-Bayes, k-Nearest Neighbors, Random-forests (an ensemble method) and Support-vector Machines.  I can use the test scoring a confusion-matrix measures to compare each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-1. Logistic regression, using C=1.6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8842794759825328"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression(C=1.6, penalty='l2')\n",
    "logreg.fit(X_train_transform, y_train)\n",
    "logreg.score(X_test_transform, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_logreg = logreg.predict(X_test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[197,  17],\n",
       "       [ 36, 208]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, predictions_logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR_Confirmed Star-Wars posts:    197\n",
      "LR_Misclassified Star-Trek posts: 17\n",
      "LR_Misclassified Star-Wars posts: 36\n",
      "LR_Confirmed Star-Trek posts:    208\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, predictions_logreg).ravel()\n",
    "print(\"LR_Confirmed Star-Wars posts:    %s\" % tn)\n",
    "print(\"LR_Misclassified Star-Trek posts: %s\" % fp)\n",
    "print(\"LR_Misclassified Star-Wars posts: %s\" % fn)\n",
    "print(\"LR_Confirmed Star-Trek posts:    %s\" % tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.92      0.88       214\n",
      "          1       0.92      0.85      0.89       244\n",
      "\n",
      "avg / total       0.89      0.88      0.88       458\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predictions_logreg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save models, splits and vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickles/p3_log_reg_MinDF2.pkl', 'wb+') as f:\n",
    "    pickle.dump(logreg, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-2. Naive Bayes Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nb = nb.fit(X_train_transform, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_nb.predict(X_test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9635302698760029"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_nb.score(X_train_transform, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8580786026200873"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_nb.score(X_test_transform, y_test)             # This modeling is also overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model is saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickles/naive_bayes.pkl', 'wb+') as f:\n",
    "    pickle.dump(model_nb, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[167,  47],\n",
       "       [ 18, 226]])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB_Confirmed Star-Wars posts:    167\n",
      "NB_Misclassified Star-Trek posts: 47\n",
      "NB_Misclassified Star-Wars posts: 18\n",
      "NB_Confirmed Star-Trek posts:    226\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n",
    "print(\"NB_Confirmed Star-Wars posts:    %s\" % tn)\n",
    "print(\"NB_Misclassified Star-Trek posts: %s\" % fp)\n",
    "print(\"NB_Misclassified Star-Wars posts: %s\" % fn)\n",
    "print(\"NB_Confirmed Star-Trek posts:    %s\" % tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.78      0.84       214\n",
      "          1       0.83      0.93      0.87       244\n",
      "\n",
      "avg / total       0.86      0.86      0.86       458\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models-3. KNN Modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "ss.fit(X_train_transform.toarray())                    # It is necessary to scale the features.\n",
    "X_train_sc = ss.transform(X_train_transform.toarray())\n",
    "X_test_sc = ss.transform(X_test_transform.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6163707304389896"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(knn, X_train_sc, y_train).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting 'n_neighbors' to \"7,\" gets me the highest test-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_knn = knn.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6157205240174672"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_knn.score(X_test_sc, y_test)                     # This modeling is slightly overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickles/p3_knn.pkl', 'wb+') as f:\n",
    "    pickle.dump(model_knn, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_knn = model_knn.predict(X_test_transform.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 58, 156],\n",
       "       [ 77, 167]])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, predictions_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KN_Confirmed Star-Wars posts:       58\n",
      "KN_Misclassified Star-Trek posts: 156\n",
      "KN_Misclassified Star-Wars posts:   77\n",
      "KN_Confirmed Star-Trek posts:     167\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, predictions_knn).ravel()\n",
    "print(\"KN_Confirmed Star-Wars posts:       %s\" % tn)\n",
    "print(\"KN_Misclassified Star-Trek posts: %s\" % fp)\n",
    "print(\"KN_Misclassified Star-Wars posts:   %s\" % fn)\n",
    "print(\"KN_Confirmed Star-Trek posts:     %s\" % tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.43      0.27      0.33       214\n",
      "          1       0.52      0.68      0.59       244\n",
      "\n",
      "avg / total       0.48      0.49      0.47       458\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predictions_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-4. Random-Forests Modeling:\n",
    "#### Since there is little to no risk of overfitting, no 'max_depth' is set (default).  I also use the default setting for 'max_features' as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross 0.8716341356003406\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=500)\n",
    "print('cross', cross_val_score(rf, X_train_transform, y_train).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(X_train_transform, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8384279475982532"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(X_test_transform, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This ensemble result is somewhat overfit, I am surprised to admit.\n",
    "#### I need to save each model as they are fit/scored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickles/p3_randomforests.pkl', 'wb+') as f:\n",
    "    pickle.dump(rf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_rf = rf.predict(X_test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[167,  47],\n",
       "       [ 27, 217]])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, predictions_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF_Confirmed Star-Wars posts:    167\n",
      "RF_Misclassified Star-Trek posts: 47\n",
      "RF_Misclassified Star-Wars posts: 27\n",
      "RF_Confirmed Star-Trek posts:    217\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, predictions_rf).ravel()\n",
    "print(\"RF_Confirmed Star-Wars posts:    %s\" % tn)\n",
    "print(\"RF_Misclassified Star-Trek posts: %s\" % fp)\n",
    "print(\"RF_Misclassified Star-Wars posts: %s\" % fn)\n",
    "print(\"RF_Confirmed Star-Trek posts:    %s\" % tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.78      0.82       214\n",
      "          1       0.82      0.89      0.85       244\n",
      "\n",
      "avg / total       0.84      0.84      0.84       458\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predictions_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-5. SVM-model:\n",
    "#### I find the best value for the budget 'C' to be the default, \"1.\"  The best kernel to use appears to be \"linear\" as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = svm.SVC(C=1., kernel='linear')\n",
    "svc.fit(X_train_transform, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.975929978118162"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.score(X_train_transform, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_svm = svc.predict(X_test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87117903930131"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.score(X_test_transform, y_test)                      # This modeling is overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This model is not the most overfit, but is definitely one suffering from high variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[196,  18],\n",
       "       [ 41, 203]])"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, predictions_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SV_Confirmed Star-Wars posts:    196\n",
      "SV_Misclassified Star-Trek posts: 18\n",
      "SV_Misclassified Star-Wars posts: 41\n",
      "SV_Confirmed Star-Trek posts:    203\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, predictions_svm).ravel()\n",
    "print(\"SV_Confirmed Star-Wars posts:    %s\" % tn)\n",
    "print(\"SV_Misclassified Star-Trek posts: %s\" % fp)\n",
    "print(\"SV_Misclassified Star-Wars posts: %s\" % fn)\n",
    "print(\"SV_Confirmed Star-Trek posts:    %s\" % tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.92      0.87       214\n",
      "          1       0.92      0.83      0.87       244\n",
      "\n",
      "avg / total       0.88      0.87      0.87       458\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predictions_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I remember to save the latest model fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickles/p3_svm.pkl', 'wb+') as f:\n",
    "    pickle.dump(svc, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The summary scores from the modeling process are:\n",
    "\n",
    "| Model | f1-score |\n",
    "| --- | --- |\n",
    "| Logistic Regression | 0.89 |\n",
    "| SVM | 0.87 |\n",
    "| Naive-Bayes | 0.86 |\n",
    "| Random Forests | 0.83 |\n",
    "| KNN | 0.47 |\n",
    "\n",
    "## Clearly, the KNN-model was the worst.  On the other hand, three separate models scored in the high-0.8's in accuracy terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue to Notebook-4.   > > > > > > >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the next notebook, I examine two of the 5 models in greater detail.  I choose to investigate the Logistic-regression model for high accuracy and the Random-forests model for relatively low variance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
